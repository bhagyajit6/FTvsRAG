{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYM5BYz_1HHn"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "bb00102b6850451282953a7f52ecd85e",
            "b0fadf6496334b2f91cc6fafceda76c7",
            "bd5293b8008a4a5c978b7377aaf638e7",
            "38a33c2bace142048970158bfff64195",
            "0da4952558464908af614d7b22ef1b2f",
            "23d2655817034f41aafe743e904ad7f7",
            "a83593b1bc2845b79d879553ec858be7",
            "c78a8f74500141e4bae7cd83c4adda4d",
            "6507b9846f704c1ca4605979854a9710",
            "88d67d64df6040358fa487ead4aaae8f",
            "ea177cec4ef546ab95bca309e9252ebf",
            "7a70f54458804c518a649fe68a7b4ef6",
            "8baf33c1fcfe493a8b59a3b37ca9d372",
            "19e7404eb9934b6dbb71b91a49bc8069",
            "eaf4ad601af74bf39e5c004fe8e7515d",
            "403dd4fc624b4637a1dc0b0d4302c74d",
            "2869dd21823442bbb8ab587f68600b2f",
            "8749f6e2ac8543a3a74e3a6dfccc2a12",
            "0f4db59ea30e4c27b7871baeb0110ea5",
            "10bffebbd1954867a6256dea45bdf854",
            "a03f5674f99641958ba268a74fed4e01",
            "7adcedddd73d4e53bcbcaa8b82611504",
            "d88cc18b8f164e24933cb05de6b84ec8",
            "c1c321e2d6484baeb484824f62207595",
            "f07b7b37e6ad4ddebc42bf3d096a2b7e",
            "6bd8cb7d05734a369646556295a781dc",
            "dbf2ef9bc83c44a79e433070d64f2907",
            "649058012d5d4f2bbd918b8f899258b1",
            "e2fffc33573f47b18652e6e3d9632bc2",
            "056020de14dd4c00b01557a11547d20a",
            "4789f4c5002e491fbb0a275f91247536",
            "cc6e08e3da2f4055890745b1a38e82d3",
            "ad1d6b83c086440d86e2e54781824265",
            "67687fd8e22f45b7a8a47f8193c4ea30",
            "929de3e335494117922a025ccbb4550d",
            "4c49ee7a2f2547da8ad30cfa72ba821b",
            "ef22e2333f034dca8bbaefca3080c3d7",
            "576c35c73259486fb87b74774d6d98c8",
            "67bc936a1db94811bb09774d1d54c2cd",
            "91763701b9344fabad5453aa62f62664",
            "bc8c9fde918a49aeaada35c82eeb05a6",
            "ba3c32d0798d48579c4f3637aef00482",
            "b58fbe564c874c1b81a59a324d28858b",
            "e446497ec05e405faf6a668d010502d3",
            "b6dad9f31cfd47a989f88e6739477368",
            "7240f021267a41f49a3825cb5dc67134",
            "51f91c72a1eb4661805f89b4f9b6934a",
            "517ee267746b462d8b8450a145378d43",
            "c10f2d83b07043919bbc54496f0554a2",
            "c018018ac1194241afacc72744b6962e",
            "357d7ff1a6f2413bade7cd18fc562bb4",
            "8b429b11b0ae4a1eb54062e3e94e6547",
            "0df0b3744ca54d6a92d1d07b901def59",
            "6ff97fb8f28e48798c0b508df0bb4d43",
            "ef888ded43584bcdbb320f5694f1055a",
            "3b79493038d74839b93b0007ea7303c6",
            "dcaead0488eb4b75a608277b535fe5ae",
            "8e5f50fe501c4dd6b1e88a28dd901a6a",
            "2180bf2d4fbe4d4d8f71971ad813f4bc",
            "4a8a9d9193aa4e3d976938718cd09354",
            "f333d3a91ac24166ae04bf3da8a426b3",
            "3197798b28a745fb8284c89a91770b02",
            "3221e549618743b683ec2a7220a031ac",
            "f5a64471c618484ea61c8a0ddcf9174a",
            "7b7a34cb4fc345788060e705458ddf5b",
            "7af9807ae24a4345914260af2b100fb9"
          ]
        },
        "collapsed": true,
        "id": "QmUBVEnvCDJv",
        "outputId": "1affd5b4-86d2-4b6c-b1da-cfa2e08517fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.2.12: Fast Gemma2 patching. Transformers: 4.48.3.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb00102b6850451282953a7f52ecd85e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.13G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a70f54458804c518a649fe68a7b4ef6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d88cc18b8f164e24933cb05de6b84ec8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67687fd8e22f45b7a8a47f8193c4ea30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6dad9f31cfd47a989f88e6739477368",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b79493038d74839b93b0007ea7303c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048  \n",
        "dtype =None  \n",
        "load_in_4bit = True  \n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/gemma-2-9b\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "08e9eb21-7a76-408a-a8fd-6350da9ea52b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.2.12 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, \n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, \n",
        "    bias = \"none\",    \n",
        "    use_gradient_checkpointing = \"unsloth\", \n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  \n",
        "    loftq_config = None, \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "4772649a52b84d9388b52e1e9c3ab989",
            "a022815d99c54265a45f250b8b43004a",
            "02ce4c274e214333bb71db71ea72b65b",
            "11523d81b9a6421e8fb6eab2ad7ca8eb",
            "19643a50f730423cababb60c6dc71639",
            "5d01490090d14c87beb71561f954910d",
            "34f44382df2040f99f480881eb54b3a5",
            "e382bc19ea9547bbad77fb6f2bce2d76",
            "76d176ea7f44477e935ed2391e2d4983",
            "1f61c7e14e2d40339b9bf0420a23c20e",
            "a19cb7bb5bb94444a7c35c265bef648c",
            "300fe4ce4a214cfc8adcce8217189c50",
            "9e4a9fc373024f0eb78a15522427ee84",
            "8db43907fc124fbc855fe3db638f5ae2",
            "8a055a1d45fd4300a0a0262ab3c615a9",
            "987dfdf516514c97b5bd902e58010f49",
            "fe97ad2c66414f5781f5d395f1ef2a0e",
            "d9d1f05f83304c449835c0adff0a486e",
            "f9c34bf2fc2f4927b75d0c995e217414",
            "6567e09aaebf48159564c15ffc886201",
            "43924333e06244a2a5ec152c7727ad28",
            "35463fe66de049eaa02beb7fd709ad27",
            "fa986efec183425c9804fc9ae6b1444f",
            "89c5f7b92aa14fc18b03a443ea3722c2",
            "35a908f3eee24b8b81f50e744f575aeb",
            "91d8b667bc4141e59775ee0c9a1c2b86",
            "9bcfaf6b24a745c2b4316c836d7f06d6",
            "03ef03aebba2426d9b816e69eb35280d",
            "5d544c82dcf144178b9b1e214fdcdbf2",
            "f09b23b94c004a80b0997a34bd14fdba",
            "c4ef54ddbe6b40179e2b46a7c0626cf2",
            "47ef405ab8e94620bea0f2e5159dcdbb",
            "132bea97176c45039cf63d5538eb04ba",
            "1be51d7f4d96483b8ed6ee6d2b15473d",
            "57119f71c40f465ba51ec5528c8b8f83",
            "c06b9ba4a67741c08d5bcef4f41f95dd",
            "ef6294f213ea467d8e0b5acb806346bd",
            "bd09e1da4ca84466870121088a2fac2e",
            "a734a3e1225d4ee8ac42b09b7d54ca68",
            "0e75073588c5458a82127898d28c96d0",
            "d50c0c0f1c5d4f7dadae7c0a8c476e7f",
            "8e09d7febae147f188b0737fdaa16666",
            "3a3bce7ed427434ab1e0d5ac22f28b90",
            "a6005491eec04aeaa734443437aceae6"
          ]
        },
        "id": "LjY75GoYUCB8",
        "outputId": "8370bc8e-8d2b-4d18-bac4-3e9562bf75f1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4772649a52b84d9388b52e1e9c3ab989",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/353 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "300fe4ce4a214cfc8adcce8217189c50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/8.83M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa986efec183425c9804fc9ae6b1444f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/16407 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1be51d7f4d96483b8ed6ee6d2b15473d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/16407 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "\n",
        "{}\n",
        "\n",
        "\n",
        "{}\n",
        "\n",
        "\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"bpingua/medquad_cleaned\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, \n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs=1,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"wandb\", \n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "3dd0993f-82bd-4f96-ee68-b48ae36dea08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "5.953 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "aa97a420-35b8-41c3-f74f-7ec2fe148fd0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 16,407 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 10\n",
            " \"-____-\"     Number of trainable parameters = 54,018,048\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbhagyajitpingua52\u001b[0m (\u001b[33mbhagyajitpingua52-odisha-university-of-technology-and-re\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250218_185039-dpif3izm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bhagyajitpingua52-odisha-university-of-technology-and-re/huggingface/runs/dpif3izm' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/bhagyajitpingua52-odisha-university-of-technology-and-re/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/bhagyajitpingua52-odisha-university-of-technology-and-re/huggingface' target=\"_blank\">https://wandb.ai/bhagyajitpingua52-odisha-university-of-technology-and-re/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/bhagyajitpingua52-odisha-university-of-technology-and-re/huggingface/runs/dpif3izm' target=\"_blank\">https://wandb.ai/bhagyajitpingua52-odisha-university-of-technology-and-re/huggingface/runs/dpif3izm</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 02:11, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.843300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.876300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.665600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.557200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.486700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.280900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.355900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.227100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.070300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "5eb97b17-59f6-47e1-b5f8-a97e08a3b59c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "362.7819 seconds used for training.\n",
            "6.05 minutes used for training.\n",
            "Peak reserved memory = 8.494 GB.\n",
            "Peak reserved memory for training = 2.541 GB.\n",
            "Peak reserved memory % of max memory = 57.622 %.\n",
            "Peak reserved memory for training % of max memory = 17.238 %.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "3241b90a-2996-4305-b0d2-327008a92bdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<bos>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnswer this question truthfully.\\n\\n### Input:\\nWhat is (are) Glaucoma ?\\n\\n### Response:\\nGlaucoma is a group of eye conditions that can damage the optic nerve, the health of which is vital for good vision. It usually affects older people, and is one of the leading causes of blindness worldwide. Glaucoma is often called the \"sneak thief of sight\" because it robs people of their vision gradually']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "FastLanguageModel.for_inference(model) \n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Answer this question truthfully.\", \n",
        "        \"What is (are) Glaucoma ?\", \n",
        "        \"\", \n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_nWQGw2o36cD"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861,
          "referenced_widgets": [
            "4ec42190f97247908d11ffa0ede7a9fa",
            "5451b37749f546d9a64cfb3527c58927",
            "7443a7675d154710ab649195d3b70827",
            "8197c66267ea44a8b374e400db6cb02f",
            "9a5989cf28c2412895ebb7542c9bb07a",
            "cea2ef822ba64ef1bf3fce2ddcc40410",
            "1465f73c0fe448daa89a28eba2facfd5",
            "8e0acb56a3ee4204bd00c58e2cb8c5d0",
            "c4c4723f910e49ee8144027d809218ad",
            "56a776b4095a489d8a130bb114f4d257",
            "22f105187f8145668a9b39ebe24bb4be",
            "07b51dc8966b48f395795bd2fa73d907",
            "dc8dc13801884ef6a4238dcc9b0eb7f3",
            "cc8ab5d93fa84bf99827904538831c39",
            "c80a9f8f0dc2423189ce539ea2fedf10",
            "5bef253d9ec14356963808955721d4ba",
            "93e21f980a6246049b78429068102768",
            "d30bace135654f358d5c7c177710fed9",
            "2b213b34e24849d78019c66b26806c9e",
            "98af1e10492b43a298d9817994f01da6",
            "7767bc89b7d148bd8f002ca86b6f4e05",
            "e2487c50976e418cb1b2399e1b028911",
            "4eb2dc870aa644a68fb27f27cff6a364",
            "001ce7c697a4455b9d9e7bdf1ec020fb",
            "d99b9a636a654d86b5d9d80aeac2be69",
            "27ec7c9501c142e4a25e9e0c6a9bb924",
            "c915ac4782694d359f3d6aeb35c53de3",
            "91422112ab484c5cb63f4edcb164ad6c",
            "13e490a06fb645b18c6f066109a76a3b",
            "455f4ba93ee54ebca6ef8c0c10bf0836",
            "c2b154cbb316448a8bd9224cda4c4262",
            "cc80f59223b647eb8ac66ed034763d24",
            "0d35a4ce237f46dd96ec06ae45cee20d",
            "e1fb9025ff964837af5e0e984e8a141e",
            "b7722d1d440046519cb9a26909a7e22c",
            "e2cc576fb3984c7b97c3dd7a3d6d4e16",
            "58f8ed55fa78418c92644a8e6c5597a0",
            "80bf5bf233d0452697230840ea2cd0ac",
            "619c3e5a9e67412c969254444b21b203",
            "5e4a694bc53643d19c530864aa32a908",
            "21eb0950d7bc4d4b90d91229ff1bb7bf",
            "ee622ec788b64b8d8ef0f420dd34acc2",
            "7478f413247242c0b00fe883b3044b21",
            "cce517dd86c74ad2ac4e40ba0f125ff6",
            "049327a7695d429db72dfe4902b2746c",
            "ad7c6301d06b4dfca26dfb8698ad9323",
            "a93245c32cd74c2fb01e3f812bdb4ad8",
            "c03a238a0bf648d9bb38257e174bdd33",
            "f8763f98faf44ca79294c103c58ac9fc",
            "d65e313b01f1487e9a82c09d084200bd",
            "0c5f69b8551b4695b6ebe23e959b53a3",
            "14c8e5f11dda48b48b594efe25bd0c87",
            "e77be99b9fe24c0ea3bc9947ccf957e6",
            "ce50e4b2ee1f42f2bb23e568f07e7a6d",
            "e7950e7aaa7f43488dcac60fcf35e329",
            "b5aa16b91e1a42d69d3f124bee4f0ebe",
            "94968c25c3704a8089b94f9d9ff67960",
            "24beb4c9eeb843629edc343bf62fac0c",
            "f871c30c28ad47dbbce6c53f919fe899",
            "656b47a2d7884f419442ec945a4945fe",
            "d5561c4137654398bfea1a484d163e74",
            "f9009dc3066c4b6da6100bd51772eabc",
            "303c41177cc74ce9bc77dc5f1146ba07",
            "107e0d13f2994a6392b277cd0b7dc0dd",
            "1368b3ab7ab8455e97ae370218c326cd",
            "873b918c12fa4bb0af24d1ec1968cde5",
            "364c40e04d8f48879f25edb6c579709b",
            "55311b1ea27c4456bf275678e722cc0b",
            "67dcd1accd9646e598ad0f37bb507a13",
            "5055e549c0d6479098b26144f1b83a8a",
            "583c237eb64f48d18b89e4db426fac46",
            "138ca4c5310e47acbd74452adbe94a22",
            "83a814fe55d94b1abc2c9cb7d36de6f4",
            "54c35ccc6d4f4b3987c6234a73a9d5ae",
            "2f2e99dd16d64f35800b03ac17ef7410",
            "0080a8d33af8463da70d945ac28c585c",
            "10d8994003f040e782045ebc6ca0d6cf",
            "08348ce04c72403e925e6bc54bbdb840",
            "62bcaf0901ac4bc38843c25920b0e5cb",
            "42f5887d17ad4edc8c42e6a2275a00f6",
            "dc762756974d432c90c88c63a753bb34",
            "ef3b25726b95431391c1f6485a9ab05f",
            "31bed2a0ec8c469aad2c33473432badc",
            "072881744b404692aea00648828c56fb",
            "95c70d30dc2c46c8acdf3893422a5055",
            "51fc881b132245b4b29a6470e65db88f",
            "37020942d29d419b96a4e75f759fae83",
            "88a3014e91f94f37be855ccff1f4eb3b",
            "d25e0bb45a8f4b6ca2874956187eb6d5",
            "6e92f4baa70e41c9b17a174d4aec0769",
            "41f9859ee22d4f86a60591b2cf46dea3",
            "222d554493cc43d7878b07679009c47b",
            "8447859b260843c48a1757444f2e4a57",
            "77ec0f409d3f403ab99ce0365c725f06",
            "11249ba85c3640c3b2baed985a7683ea",
            "be3a2e4dc3774f5087f3a236f7dc88f7",
            "9425470b3ac34aaa832fbd34af233c1a",
            "39b1445f7a014d7ebbe6bad4f8d682d4",
            "727cb7a71a7c48d3bb622ac27f9c1966"
          ]
        },
        "id": "fr1FQJmo3_6v",
        "outputId": "a5af3a26-f6ec-4739-ec24-cc6955ceda3d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 6.1G\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 4.31 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|â–ˆâ–ˆâ–Œ       | 11/42 [00:01<00:03,  9.80it/s]\n",
            "We will save to Disk and not RAM now.\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [01:52<00:00,  2.69s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving gemma-2-9b-medquad/pytorch_model-00001-of-00004.bin...\n",
            "Unsloth: Saving gemma-2-9b-medquad/pytorch_model-00002-of-00004.bin...\n",
            "Unsloth: Saving gemma-2-9b-medquad/pytorch_model-00003-of-00004.bin...\n",
            "Unsloth: Saving gemma-2-9b-medquad/pytorch_model-00004-of-00004.bin...\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: You are pushing to hub, but you passed your HF username = bpingua.\n",
            "We shall truncate bpingua/gemma-2-9b-medquad to gemma-2-9b-medquad\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 3.92 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [02:04<00:00,  2.97s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer..."
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ec42190f97247908d11ffa0ede7a9fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07b51dc8966b48f395795bd2fa73d907",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4eb2dc870aa644a68fb27f27cff6a364",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Done.\n",
            "Unsloth: Saving gemma-2-9b-medquad/pytorch_model-00001-of-00004.bin...\n",
            "Unsloth: Saving gemma-2-9b-medquad/pytorch_model-00002-of-00004.bin...\n",
            "Unsloth: Saving gemma-2-9b-medquad/pytorch_model-00003-of-00004.bin...\n",
            "Unsloth: Saving gemma-2-9b-medquad/pytorch_model-00004-of-00004.bin...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1fb9025ff964837af5e0e984e8a141e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/576 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "049327a7695d429db72dfe4902b2746c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5aa16b91e1a42d69d3f124bee4f0ebe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00001-of-00004.bin:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "364c40e04d8f48879f25edb6c579709b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00004-of-00004.bin:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08348ce04c72403e925e6bc54bbdb840",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00003-of-00004.bin:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d25e0bb45a8f4b6ca2874956187eb6d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00002-of-00004.bin:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n",
            "Saved merged model to https://huggingface.co/bpingua/gemma-2-9b-medquad\n"
          ]
        }
      ],
      "source": [
        "if True: model.save_pretrained_merged(\"gemma-2-9b-medquad\", tokenizer, save_method = \"merged_16bit\")\n",
        "if True: model.push_to_hub_merged(\"bpingua/gemma-2-9b-medquad\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
