{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sVDZwq7WGLU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316,
          "referenced_widgets": [
            "ae0a024f9ac747b28a88c57ca06ccaab",
            "975104a5c3ab416191a49493e1daf5f9",
            "75805cbf05d343c0b03618faed8ea41b",
            "507ca5ccade34b59abd7d392b54d759e",
            "47bd83a1602d4d83ae1548e63aa62aa7",
            "f32b0e1ba32a46659fe6ca08bcbc370f",
            "e89c8fdc4293402d93471b05115a7d62",
            "65aac4b5d0184f909798c7e77317869b",
            "69674c13b48949bda0c4d016c10bf6f2",
            "3c3875f65bca4550a16c6e9bd265f0bd",
            "cd5f727399474476a1b3a4b9e73f3297",
            "d865f7d5f28541a2b0d0f12ab7c13637",
            "894a8db1fc1940e1acd5ce612c5d88a3",
            "29a582eacb1e405dbbda0f0cfa03a458",
            "8968d149610a451e90756b8ba03fa037",
            "65a079ad186449cc96d498328d1b74ad",
            "47aa4b9cecf543a9bdab9762cce1502a",
            "7eb85b4404994c98a2fef17624b901f3",
            "6810cbd94dfd4f588d5eaa58cd899818",
            "077f3855aefc4dcdb7e40bd7c37472cc",
            "93e5e146db9b45dd81f565d4b6a6420d",
            "b73f03c3455f4f56adc4825f9aac6cb6",
            "5f1cc73d328647239be7c49beb7f2667",
            "c26133364e3b4fa595d199d0b777514b",
            "4c57394db0ad498485538d463e2f6a24",
            "4ea04604ef3f478f8e668320bd5c6d1d",
            "c721a500398e4553bc46c22bf3ddc2e6",
            "170615bc02584e12922f3e9bddae8ad9",
            "84ad63922b0544ad9281d8f3922da671",
            "7ea1711cb158427499cbb0082cf07e85",
            "2070284260824f8399183440cdb93820",
            "1a453fd071cb4d25816b9261c4ffe047",
            "7dac28f2b9f94f9d89bf347037878ce2",
            "0fbdb94fde6043a9b7654d3d5fc3d296",
            "ae636ea402404857b69c953c54400bee",
            "45658e058d524af59004caad314f0637",
            "b972d1e9ac584b3da80f460c7e307b0c",
            "5724ca279d704b51b66381f55dae41ea",
            "b5f06a3b1eea40b98633388509d891a3",
            "144697e0eeea4fb0987de5b0501c08d4",
            "7350501bc05c42bfaa5be6e956838e2d",
            "8964d800f56c4a12ad80828eb4e781a5",
            "b55b7eda9ba64e1fa4b97f4b04d08c3c",
            "42e86d57539342d79a393fa62431d6ba",
            "982fb932b75d48e6bf401abf02bdd486",
            "8c71eb0da51340558946e1f5829912dd",
            "9a3c635225034614a5b6ccad1b7860da",
            "85a880a627164040b8a1a10f54d77cca",
            "2aeb37d16f81432289666806271dfaf7",
            "99e9c842aa7044f09e4f4e4718543c9d",
            "8fdca62e2a6f4af79f14615f34449dba",
            "6c0bc6eba5654be687c314c484ab7646",
            "2c2123cac84242b4a2d263d3beea77a1",
            "687449a8b93d40438a6aee4acca9c5b1",
            "c3143939aeda46f7b576058511a2b655"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "0763050c-8aad-4d64-e48e-7f4a61709084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.2.14: Fast Llama patching. Transformers: 4.48.3.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae0a024f9ac747b28a88c57ca06ccaab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d865f7d5f28541a2b0d0f12ab7c13637",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f1cc73d328647239be7c49beb7f2667",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fbdb94fde6043a9b7654d3d5fc3d296",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "982fb932b75d48e6bf401abf02bdd486",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 \n",
        "dtype = None \n",
        "load_in_4bit = True \n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "98bc606f-c6b4-4d71-bd6e-b11dffeef379"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.2.14 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, \n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, \n",
        "    bias = \"none\",    \n",
        "\n",
        "    use_gradient_checkpointing = \"unsloth\", \n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  \n",
        "    loftq_config = None, \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "bd281f439ef946929c2192b9a46975d8",
            "da02dfeb37c742b38a6ab736da81b95c",
            "b71f7384fb8749549aa17832fff779ed",
            "1f4614d575f442d4b2d3a17c9ead989a",
            "c8edde050ba1470181a0f5616a6aabe8",
            "5776f9accce2423eb2dd35848d5a872c",
            "b46189eb6637406ebfb3026ae0af9314",
            "43775e19d1024764a9c13d3eaf27e314",
            "0f0262be3f874a42a9533f324d6765de",
            "f6140b7f2944436c84b5705fb07d4967",
            "36ea55f5292042f7a66ed9c64bca298b",
            "b73263bef82c40d7847bce0807ba1722",
            "fc05f97f42044fa9ae14bb231a3a95c3",
            "49530e30b8e646108261c67016c6a49b",
            "341132e7f6444dee8c3ff96767ec05a1",
            "3c1a57d030eb4a2f83a28ca25149264c",
            "d52083f2fa72430aa337ec831ef05c9d",
            "5cb7492811224dc68291daa08062005a",
            "46b3e01025cc47e6b91c69cc0a7a8e96",
            "d6dbfe1bb7904478b49541e9f545b4d9",
            "9535cc93cd6445e9b55f7599730a9b52",
            "0f8e5983deef437fa0bdf2e9e18557ca",
            "8e5b05d5a9c04160826a602eacbf50bd",
            "815beefb34d14f7a98bbc16c1a787f4f",
            "b146d2dd02fd4c68bda610ad38d37a3d",
            "7513964a6b0d4816ac5b04da0f58ba15",
            "c1f02d251d994a7b8dcb98cd8e558f20",
            "a0b929a57e8e49bc8b1a898bc8222268",
            "6fcc7adbc3ac41e9bf2212ff2101f829",
            "8fb02637bcff4a8d8dda20eaf8def2f4",
            "4ccb57578c604c019856636b0a42c228",
            "ae25cde2fe2b491f80b296b40b3eba85",
            "92d2c5726d65437fa3f07a281d995dba",
            "d02ab607b3694eeda0d847b8c6233a8f",
            "623bf40de7d043398dc051bea1d4099b",
            "4cc5ca4bb2564769b07ee1fca5e31c78",
            "75fa8c39066d4da7b19087d78a599278",
            "70338e30225c44ee88cb64e7ed67e51c",
            "6f743f5f9bf14803ad8ff223a47592ac",
            "63371a833de24733a90822b23a00d4db",
            "802847e3e47040f2a52e1dcd39b90afb",
            "93f70b0c87b7476fb69d7ca1bd5fe4b4",
            "538d9c9c27074b23a7dabf87c535c723",
            "ba9834cd65b8421fb78c2bc2b299faa7"
          ]
        },
        "id": "LjY75GoYUCB8",
        "outputId": "1a35ef89-3fd8-48cf-9fa4-cd2c5b3f2eae"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd281f439ef946929c2192b9a46975d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/353 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b73263bef82c40d7847bce0807ba1722",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/8.83M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e5b05d5a9c04160826a602eacbf50bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/16407 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d02ab607b3694eeda0d847b8c6233a8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/16407 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "\n",
        "{}\n",
        "\n",
        "\n",
        "{}\n",
        "\n",
        "\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"bpingua/medquad_cleaned\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "4c3a86df555f4e5ab4fc98af41c5f60d",
            "ef26a4db8c73430da64f2efbd2ce7925",
            "46846b5560a74274a7704164785d3235",
            "6d194f67a7444b5dae4fe53d5ee26771",
            "f354519b657d4b09addb46b50651773f",
            "e3c49789a1784c8088d1564081cd6f3d",
            "f8c71efc658247139a5c9cc49771be40",
            "120f3dc846404128b274b5fe1e994bca",
            "92cb6dda01584017b6fc8acebd1752c3",
            "2a03d741143a4a2ebfbb6c77972a6f4f",
            "6835671bd6214a5a8eb1fe4e55e37288"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "cc79dd07-2258-47c4-ffb7-2f4e582ae601"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c3a86df555f4e5ab4fc98af41c5f60d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/16407 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, \n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs=1,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"wandb\", \n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "d397dd48-304c-4f42-ecbc-d5c9ce14989c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "5.984 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "a5763cca-0720-439b-a003-ae812afe1ae7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 16,407 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 10\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 01:39, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.808700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.720600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.543700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.549600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.496600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.457100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.212600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.439400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.305400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.117100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "edf33a96-b12c-4bba-9771-59e18aee707c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "462.7198 seconds used for training.\n",
            "7.71 minutes used for training.\n",
            "Peak reserved memory = 7.922 GB.\n",
            "Peak reserved memory for training = 1.938 GB.\n",
            "Peak reserved memory % of max memory = 53.716 %.\n",
            "Peak reserved memory for training % of max memory = 13.141 %.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "cf733ee9-37d2-4f3d-f207-fa8ddd59d0ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnswer this question truthfully\\n\\n### Input:\\nWhat causes Glaucoma?\\n\\n### Response:\\nGlaucoma is a disease that damages the optic nerve. It is caused by increased pressure in the eye. Glaucoma is a leading cause of blindness. But blindness from glaucoma can usually be prevented.<|end_of_text|>']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "FastLanguageModel.for_inference(model) \n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Answer this question truthfully\", \n",
        "        \"What causes Glaucoma ?\", \n",
        "        \"\", \n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMjZW1-xWX9M"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828,
          "referenced_widgets": [
            "77d55a08d9df482c824f8c1e8f938d98",
            "56761ceaf60641e6974bd389820c9ecf",
            "0af6c946de0e4a26bf2cfbba5f0248d8",
            "6d9c94245bed44a2aee878785a9436fe",
            "181aad6711fe4e6ead7cfbc8dbb47dee",
            "d3f40a91f40b4e25a2ed26e3eed703e6",
            "36a0e8be5ce94141b434e661207ef670",
            "fc18f7410aa34f1c992fe11e66092121",
            "006bb5d1d9524bf2b2fe138262b0235a",
            "d3983858887749fea29a7ad9cf72f999",
            "31a953c0266b444c8073bf7a61048a93",
            "4df0b2b2b0a244ceb63a3e8379cabbe0",
            "8bc7cdbe3baa4725a881f41a8378ec54",
            "a36a556c35dd45e686ba1c2da7b81eee",
            "8f71d7dfc3884c81b1b424335cef5572",
            "2275d603438a4de48efd73e2fcde4879",
            "b1d2860f58704efba33b2c32319ad8aa",
            "148dc37da5f84b0eac7191753be7ba8f",
            "bd3834c44cab4f8894c37bfb5421bfbb",
            "30959cd3e2f343c49f3e979b10be70bd",
            "31206419771b46cebbce6c0624d20d40",
            "f7937950ad874234a6950196c2c92f3a",
            "80e4016b2ade458dbe24c67be2ee4886",
            "8a46a38d8d38426e9ba8eefa43988f85",
            "1a3c84d561094fe3b760fece662c3d5c",
            "4d73b098c6a64adb8116f9f73bc7506d",
            "2f60a24471724098b8f15f4da9342d3c",
            "9a4e4155190a496f823ab6c373dca623",
            "e348ca9997f74169a78966b71dc2f7d1",
            "3ea83e4e5efc4351aa9859497726b165",
            "2af4a13774984fa8a65067bfb6832b08",
            "564acb8865814e1ab7352d469687bc20",
            "93665d0a409d46ecaba59cb15561a544",
            "362ea8c1fd6c4acf97a49be8b7e64eb1",
            "41cecaa942644cae90680f0988d8022b",
            "14e150a26c02487ca3844e064e95f401",
            "f022937b200b4348bc6c631813f867bd",
            "28fdabd583254be6b151d8197d06a76d",
            "25232953058947af94a542a000093438",
            "c26bf111343a4019931657be7bd27755",
            "8c84530e6a5f469b9ae4cbdb123e5b7f",
            "9128130877e6401c9ebb9b21a7e8570b",
            "bc07316980854f01a78b6a3fdeac7861",
            "89b357a413884d65b1196b29872af916",
            "b9423338dd904447b04674ee8253a129",
            "f91eb6ecfe054427a7e0fbca53bf1a5f",
            "1049aab124ee4e6cbc64744681f38c0a",
            "2a13ce995b714db89cbfdbcbffbee83d",
            "7bafa15d75cb445f9907d83202d1f603",
            "63f423e2f4f649c6b13db7918b28e3f6",
            "5d27bd4345fa4c53bdc0471699f5706e",
            "b84d476f32634b69b29550853517fd5a",
            "91c3b763f7634c239685c0770e1f3b49",
            "79e31a15624d4ce6ad6ab130a18bd49e",
            "7f7c30b384024d459552a35a0663238e",
            "ac178481dcd14fe28b284d812068605a",
            "32e941a4b3fd406dbb1af45b9b274daa",
            "691fdb3f0f914479b650897f5cebb86a",
            "4d91f46828824cfb9dd94048e8f0cb29",
            "680a888a0cfd4d4e8982eafddc45279e",
            "b2557bb4535f4ac09dcdbbb635ae81fa",
            "31399a30873f4517991eac1a7b636458",
            "e720d5e46091469b97583b6030d9da13",
            "3b59d855e9c54954b068ae3a384a186e",
            "8979155143084e40a83d0607d64c7f2b",
            "4b076f5ff1464089b93a1396c297f17f",
            "48a1a603c07142b098806ade247267a2",
            "7cf4af99539e4f648f833b8338acc06e",
            "b7041093795c45c196c719cfb4307e4b",
            "0d9d47e14d1a47a7af5778e2a2678007",
            "1826c91af2a243fabc0e5c16d681ae70",
            "27c3310baab54674ae590fdecd5ecd9c",
            "a5908cde2a5a4afb9158f51afaff6f00",
            "e0c7bd29c029413cba341c03e5273240",
            "be3fb8c253ed40fdb7ce42c2f65bb018",
            "5cfa2bf0cb5747308d1ba63fdd8d1e0b",
            "a8ad8da229fc4c9fbc5fec61c2b12b7e",
            "065ec4e0b29b42e68735f7fc394d2834",
            "8b8650d52d0f4d71a7ba29a12844d9f1",
            "82cad378a0994c52bf03b13faa1777d3",
            "98752591e33d4c17a696a44a339d5e14",
            "40cf712678f34e6dab940c23fd9e49d7",
            "0fab11ced9a54b33a414771e303a0c2a",
            "590ba50d062c49158031a01dc60c274d",
            "2f12248d14be46eb97e1aa2847f96e73",
            "3fb51a1dcf4d435e9a52e41581c5257d",
            "b34495a859f141a5875bce6d2e26977c",
            "9cceffd333134dffb5598fcac1499a28"
          ]
        },
        "id": "Y_gjQ5kOWbyT",
        "outputId": "20a03db8-8b2c-4615-b10f-05c09c61b70b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 6.0G\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 3.93 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 11/32 [00:00<00:01, 13.63it/s]\n",
            "We will save to Disk and not RAM now.\n",
            "100%|██████████| 32/32 [02:17<00:00,  4.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving test-Llama-3.1-8B-medquad/pytorch_model-00001-of-00004.bin...\n",
            "Unsloth: Saving test-Llama-3.1-8B-medquad/pytorch_model-00002-of-00004.bin...\n",
            "Unsloth: Saving test-Llama-3.1-8B-medquad/pytorch_model-00003-of-00004.bin...\n",
            "Unsloth: Saving test-Llama-3.1-8B-medquad/pytorch_model-00004-of-00004.bin...\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: You are pushing to hub, but you passed your HF username = bpingua.\n",
            "We shall truncate bpingua/test-Llama-3.1-8B-medquad to test-Llama-3.1-8B-medquad\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 3.92 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [02:57<00:00,  5.54s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer..."
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77d55a08d9df482c824f8c1e8f938d98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4df0b2b2b0a244ceb63a3e8379cabbe0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Done.\n",
            "Unsloth: Saving test-Llama-3.1-8B-medquad/pytorch_model-00001-of-00004.bin...\n",
            "Unsloth: Saving test-Llama-3.1-8B-medquad/pytorch_model-00002-of-00004.bin...\n",
            "Unsloth: Saving test-Llama-3.1-8B-medquad/pytorch_model-00003-of-00004.bin...\n",
            "Unsloth: Saving test-Llama-3.1-8B-medquad/pytorch_model-00004-of-00004.bin...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80e4016b2ade458dbe24c67be2ee4886",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/604 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "362ea8c1fd6c4acf97a49be8b7e64eb1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9423338dd904447b04674ee8253a129",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00004-of-00004.bin:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac178481dcd14fe28b284d812068605a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00002-of-00004.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48a1a603c07142b098806ade247267a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00003-of-00004.bin:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "065ec4e0b29b42e68735f7fc394d2834",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00001-of-00004.bin:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n",
            "Saved merged model to https://huggingface.co/bpingua/test-Llama-3.1-8B-medquad\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if True: model.save_pretrained_merged(\"Llama-3.1-8B-medquad\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if True: model.push_to_hub_merged(\"bpingua/Llama-3.1-8B-medquad\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
