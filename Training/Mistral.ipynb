{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu9UxH61MbzZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "283d89f79f8248c9899aed3db23daba7",
            "ecb5b4b9116847a0aa0c58a8a165117e",
            "799990c4c0a24a6ebd00b35c62b4f18c",
            "1151046f850642ed9036abeff6ba3bad",
            "aa300b1185b744548aa85bc254eb0719",
            "ddff775e129c4e998e50bc1c99996558",
            "3da1cb7c534c458f82f6abb3c68e8547",
            "df67b0f5383942b1a2dbe3ff4340cf7a",
            "2c3dc28f999a4141ac0c31ec9539476d",
            "1753efd514d7417eaed0fa4e40c5a73c",
            "8a6f6f82577345ea853fb25a368798e9",
            "72db4f158a7142d9900faf1e9d9acda5",
            "37bb054f768e41e5a18bee7a63bbd47d",
            "36a16729bc5440ddab3ec3811258b155",
            "a86de06ee4174daca4299338bd51c9f8",
            "561ab5d201e4465d9acb413b4c177b4b",
            "a8b59e6e872c40edb9ed7feb90b55e0e",
            "8f73cd53149741a3acf93050aa392d3b",
            "648b4b1b6e504e3fa439112b2ee16815",
            "7eccf088061c4987bbed4ccb61d0ff55",
            "0b7c3fb74d2442fc91a5c67f6d0332a9",
            "5143d5c912ce409d9a58981027934fd0",
            "ebb3cb59119c4e219605e99f2a64548b",
            "022e26c92110467daf3a472354c885fc",
            "1c0e80fa7cfc48ff82dd75f66180f2d2",
            "b3d5ab1344c844cb8b2ff8a121a8f227",
            "b9bef20e4243436db8c82cf70c91794c",
            "0bb4e499a8fc4be0b3e68b3fa9739f76",
            "baac6514cd5f436cbf3057f9512d1d68",
            "efe08efe785446ae8b0d083e76ff92aa",
            "a421b9add9cd4b91b4516a2998f89fcf",
            "83477105624a4a5fb1369a4ff7cb39d2",
            "d5de143f5c5b42d5827560189807e38c",
            "7078876496134b98b3397711df611503",
            "17da2483412c45f08eacab9d6e9702d8",
            "6cfc523d6fdc4820bf52c2bd33a1c5cb",
            "b7174f81d40344a3a2da24e988775fb8",
            "191b458fe137439195b151f763fa0152",
            "076799e4ccb3407ba20b6cd3ad479990",
            "1dbbee03815142359b19035b8527a713",
            "bc2e257d4bd6456598d22864c4aa12b3",
            "c163521e465f43a7891c74ff77886518",
            "9c37dc327d514c90aa20d21fb4f2c4d5",
            "be9e49aae2c946bbb9981f0999615c02",
            "f1de0a96c18a44b2bc05d646a57d2890",
            "916f70d8c1f9491ea6c90a5d30362dec",
            "4a33df3d5f1b49b98dde5126ddef23ec",
            "25a1dc08ae054c479ce6a0a89b9ecc9b",
            "8a5e055129e54139a5229d024c286238",
            "e6c3c07f0dbe4adfa505c3d8f9165282",
            "7119d0af0cc741c2b0849f6e18bb5b80",
            "d28d7d7dfa0a4f37b437e9b68d137b32",
            "510f710fa84f49ad811ff9853e20e056",
            "f46cd886545b40f197cb1f9217a956cf",
            "fa13d76d05f4452fb6ba95a85b05758a",
            "5ca4356a83654a9ca0a9cab0fa934ed8",
            "b2b66a5027b8425185a4e9aaf4980586",
            "b87e02ebdfee4c92a9acc78ce6be4424",
            "b0c43662f3d74f7d82ef5bad418634be",
            "d2987a7dcea542af8c3433d82e91ffed",
            "b825fa96668c49e98bc5109291e2a1cf",
            "f27eacfc8679494dae5263094cbb3954",
            "64f1761a1f3345d2930f5ea11255312f",
            "a50e72a163d840bea85784d995c0dd76",
            "bb54247e78ef40e481c705402828de55",
            "a3ac17efd4344da5bd58cb65baedf6b2"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "f6b5759c-f68b-4c0c-e56e-cb915483e579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.2.9: Fast Mistral patching. Transformers: 4.48.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "283d89f79f8248c9899aed3db23daba7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.14G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72db4f158a7142d9900faf1e9d9acda5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/157 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebb3cb59119c4e219605e99f2a64548b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7078876496134b98b3397711df611503",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1de0a96c18a44b2bc05d646a57d2890",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ca4356a83654a9ca0a9cab0fa934ed8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 \n",
        "dtype = None \n",
        "load_in_4bit = True \n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", \n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "bd956aec-53f8-4dcb-c420-c2a95c0f7029"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.2.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, \n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, \n",
        "    bias = \"none\",    \n",
        "\n",
        "    use_gradient_checkpointing = \"unsloth\", \n",
        "    random_state = 3407,\n",
        "    use_rslora = False, \n",
        "    loftq_config = None, \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "c64104bcb83d49a8acf0834087fede61",
            "b3d321190b714d90bc8de11efa79d172",
            "4fe8b4c5414e4cfa9ee734f6aa7643b4",
            "f784ff26b3df45528cfb0e9d405ffb7a",
            "caf96be2b2694815a2492246fa00a213",
            "7613fa08153a4f0fb9777f37d5b149fa",
            "1453c85acc234f7598a5923560309837",
            "d60e3ca6bce5487dbec305830b521c59",
            "78dbc5e1bdad4c788bfc8d0f0ac1d814",
            "121b743283144cb8a97fc789b52a6bc2",
            "efad5a6710654cb1b600faf3562bfe65",
            "60022ab1170a4642b0b14f1d734cea10",
            "167b820bcb82411e8e827cbf070ed747",
            "3ab741446c6140778649c8545ef39249",
            "35ee08bb88394d8694d7498625b31538",
            "d23ba2bf9b884c1181e29fcb1dffb826",
            "6e6ed01eae7a486f98d818aaa6322350",
            "fc5a42f9f8dc4e43a48ff4e00a45fba9",
            "36103271ef7b4062bf63b4af52199686",
            "e11c9143c0904c7aaf6902d5ae9d33ad",
            "aeedcec0f06c415594a8cbb312c30bc3",
            "76033b3272f1471b8902e9d7f760922d",
            "8d65e16150474442839ad7b2ead0e128",
            "e028c508c2614339a2c40268d3249b74",
            "621002c24d774b708148367d6985963d",
            "c481244724b74e5882ccb3ac92b8ca20",
            "517ea8839d9946d588a0b6c67c721849",
            "c4ec97bca1c94ab38dc02d4c15afdcab",
            "5697be14b66a4a50b71b0a3c59e20f4e",
            "17ce1a521f674e56a85bf1948b2d509e",
            "843013c07a6e4f4e94724eeb42b30831",
            "6b0c81d47d8d4149982aea17382ac795",
            "969aec3e456e4627a0e571a80d50c310",
            "702e44bab0b143d3b28638b00fb00078",
            "792a132110444f93882aa4a426149720",
            "d7fff03de341490486136228d60efeec",
            "ae0b2a37d8884f9683e433bae2cc012a",
            "aa42f7e4717f451885fc334a429e758e",
            "e59216b5779d4a519a9b221d91ac2931",
            "841c4d8ebac54c438c0d175acc07f6b8",
            "fbef8a7b767d4195b7e23c625030dede",
            "9f1eccee069244eeb047cc67fa7ad5c8",
            "a6b25cb7fc014dd5b891ea8ed59d8e0c",
            "e74bd65543114debbdf22bcc351d4a6d"
          ]
        },
        "collapsed": true,
        "id": "LjY75GoYUCB8",
        "outputId": "8f1a5f7b-c99c-4f64-c660-072f9b0ce514"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c64104bcb83d49a8acf0834087fede61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/353 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60022ab1170a4642b0b14f1d734cea10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/8.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d65e16150474442839ad7b2ead0e128",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/16407 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "702e44bab0b143d3b28638b00fb00078",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/16407 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", \n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, \n",
        "    map_eos_token = True, \n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"bpingua/medquad_sharegpt_cleaned\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ3SNz5CUgDB",
        "outputId": "2d929faf-e1d1-4b7e-a438-7d9e1fb68109"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'from': 'human', 'value': 'What is (are) Glaucoma ?'},\n",
              " {'from': 'gpt',\n",
              "  'value': 'The optic nerve is a bundle of more than 1 million nerve fibers. It connects the retina to the brain.'}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5][\"conversations\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWF9FDYOWn8r",
        "outputId": "e7e45a73-ce66-485a-9b1e-7e95f596066c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>user\n",
            "What is (are) Glaucoma ?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "The optic nerve is a bundle of more than 1 million nerve fibers. It connects the retina to the brain.<|im_end|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(dataset[5][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os4qBfB_XwcD",
        "outputId": "5778f2da-b868-451b-fcf3-a555d31e9a56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['conversations', 'text'],\n",
              "    num_rows: 16407\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "aa8773ab79814598abe42c63ffeff817",
            "6827fcf18fff42faad52e2285922c7b4",
            "46d67527293243eeac8c01fd27f45237",
            "2703260319274168869072d08b56c91e",
            "be3c722990584527a761848e79832fac",
            "ba5615e2341c4d0f9c94a5888eb8ca94",
            "d2ae7d879ab04768b69eef83450a34ab",
            "f0e547e5f6f94d4dba2b2e3addf0ec10",
            "72cb152176104e3f93a542de9808b0ed",
            "44b49eb7992b4c5cb8cdab3528f337ca",
            "85939e8bd2e04c44ba76ee38cd898a46"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "2b602ab4-109c-467f-e961-57ebc5c97e91"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa8773ab79814598abe42c63ffeff817",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/16412 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, \n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs=1,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"wandb\", \n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "6f467795-5fb6-4b5b-d285-a1bb5831aea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "5.785 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "0f6cb8c4-b866-4ffc-a0fe-40cd3b13d0f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 16,412 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 10\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 01:39, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.425000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.722500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.655000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.406000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.421600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.305700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.253500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.218800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.169600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.089400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "ec911e2e-712f-4f62-9b95-75ee5d605d01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "141.3359 seconds used for training.\n",
            "2.36 minutes used for training.\n",
            "Peak reserved memory = 5.785 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 39.244 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "37051a8e-87f6-4fa5-d415-97310cb55535"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<|im_start|>user\\nWhat is (are) Glaucoma ?<|im_end|>\\n<|im_start|>assistant\\nGlaucoma is a group of eye diseases that damage the optic nerve and result in vision loss and blindness. Glaucoma occurs when the normal fluid pressure inside the eyes slowly rises. However, with open-angle glaucomaâ€”the most common formâ€”there are no symptoms in the']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"chatml\", \n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
        "    map_eos_token = True, \n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model) \n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"What is (are) Glaucoma ?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, \n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnjn4M_ENKcU"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoY5mdQ5Q24V",
        "outputId": "d455c067-b7d7-4c8c-e89b-af6677206d60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341,
          "referenced_widgets": [
            "89cffbcd105e4a028bfccd63edc4980b",
            "e5d02d0898344c26bc81ef9aa6906d5e",
            "590ebc8ec22d487ca23ab789436340d1",
            "5fd58f98d7f542b2b12ae952b29e2cd0",
            "d76926fe28dd42339c2284314ff418aa",
            "23587f8c2038407da8d6673732136430",
            "abcd884082584b9ba5a0a3af6349f2be",
            "d4a8e3ca24844b628133c2953deda7db",
            "317cf14dcdb3420d8dd6df48649ce98e",
            "0b411b1d0c6e42399d26681119625255",
            "1f4d3c63fe884593a90eb6447fbf75af",
            "227e1f0b45af46debc15efd8b2217a5b",
            "e98aaf14118548529f15e7750fc9c47e",
            "58e8f95dbf50472eaa16cf0bf1400bfe",
            "e4d556398cf24f22be44f3445bc3ce55",
            "03cc96b7c2094445a15e3008f13994b9",
            "7e07be24ae1d4e669483d88d1353c4d8",
            "2a44582024534bbeb717533fdf9eedac",
            "dfc7b76307a44e029e87a2679788d248",
            "09800ba75fd24fcd8842994ca2d3d8bd",
            "1c5ff2d59a554bdc86f54aa4f7466dec",
            "963c621cc8af4f019122a1d7c8fe867d",
            "473430ba2b234aeca3b862397a71fe4b",
            "8b5617f38e3a48909b67b17e621dcb17",
            "107db14272c54912acbddfb5f092b22a",
            "16b00aa9f8904e0680d3a5b2dbc3bf1b",
            "e8a5511ab577432cb62227d8288e9b1a",
            "73b81677353d400ba9f70d7d832e8f3b",
            "dd1d66ed82934d77bc6aaf6acee89f14",
            "bf5fd785558245f3a230586b140e4901",
            "21f5ed5dc2864c8eaf82587d3c3d2c94",
            "11d26607ed1044ed89bb19aee51e697f",
            "00d12b1ce9ac4f8681f5433d79366fcd"
          ]
        },
        "id": "YFmPxt1lNOOs",
        "outputId": "d878067a-47ca-4a6b-a81f-3c0fc4b1c54b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 4bit...\n",
            "This might take 5 minutes...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n",
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model... This might take 10 minutes for Llama-7b... Done.\n",
            "Unsloth: Merging 4bit and LoRA weights to 4bit...\n",
            "This might take 5 minutes...\n",
            "Done.\n",
            "Unsloth: Saving 4bit Bitsandbytes model. Please wait...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89cffbcd105e4a028bfccd63edc4980b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "227e1f0b45af46debc15efd8b2217a5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.14G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "473430ba2b234aeca3b862397a71fe4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/5.19k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved merged_4bit model to https://huggingface.co/bpingua/mistral-7b-instruct-v0.3-bnb-4bit-Medquad\n"
          ]
        }
      ],
      "source": [
        "if True: model.save_pretrained_merged(\"mistral-7b-instruct-v0.3-bnb-4bit-Medquad\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if True: model.push_to_hub_merged(\"bpingua/mistral-7b-instruct-v0.3-bnb-4bit-Medquad\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
